### Hi ! Here is Brian Qu *(officially Bowen Qu)*!  👋

#### 🙋‍♂️ About Me:

- 👨‍🦰 I’m currently a Master of Science candidate of Peking University (PKU). <img src="https://www.pku.edu.cn/pku_logo_red.png" width = "50" height = "40"  align=center />
- 👦 Before that, I received the Honours Bachelor, Huazhong University of Science and Technology (HUST).<img src="https://upload.wikimedia.org/wikipedia/zh/thumb/a/ab/Huazhong_University_of_Science_%26_Technology_logo.svg/1920px-Huazhong_University_of_Science_%26_Technology_logo.svg.png" width = "55" height = "40" align=center />
- ❤️‍🔥 Now, I am intersted in Multi-modal Learning especially MLLM.

#### 😋 Projects:
- 💥 In 2023 summer, I take part in [OSPP(Open Source Promotion Plan)](https://summer-ospp.ac.cn/) Summer Camp <img src="https://summer-ospp.ac.cn/vite.svg" width = "60" height = "35"  align=center />, with the honor of contributing for [MMPretrain](https://github.com/open-mmlab/mmpretrain) to build prompt-based classifier. <img src="https://oss.openmmlab.com/www/community/mm.png" width = "60" height = "35"  align=center />
   - Now, the implement of zero-shot `CLIP` classifier has been merged to the main branch. [![Codebase](https://img.shields.io/badge/PR-Link-pink)](https://github.com/open-mmlab/mmpretrain/pull/1737)
   - The implement of `RAM`(Recognize Anything Model) has been merged to the dev branch. Welcome to use the gradio WebUI to test it on MMPretrain! [![Codebase](https://img.shields.io/badge/PR-Link-pink)](https://github.com/open-mmlab/mmpretrain/pull/1802)
- 💥 2023.11 - 2024.5: `MPP-Qwen-Next` is released! **All training is conducted on 3090/4090 GPUs.** To prevent poverty (24GB of VRAM) from limiting imagination, I implemented an MLLM version based on deepspeed Pipeline Parallel. The Repo supports {video/image/multi-image} {single/multi-turn} conversations. Let's have a try! [<img src="https://img.shields.io/github/stars/Coobiw/MPP-LLaVA" style="width: auto; height: 20px; vertical-align: top; display: inline; margin: 0 2px;">](https://github.com/Coobiw/MPP-LLaVA)
- 💥 2024.9: We release `ChartMoE`, a multimodal large language model with **Mixture-of-Expert connector**, for advanced chart 1)understanding, 2)replot, 3)editing, 4)highlighting and 5)transformation. 
- 💥💥💥 2024.10: I am really fortunate to be involved in the development of `Aria`. `Aria` is a **Naive Multimodal MoE** model, with *best-in-class performance across multimodal, language, and coding tasks*! [<img src="https://img.shields.io/github/stars/rhymes-ai/Aria" style="width: auto; height: 20px; vertical-align: top; display: inline; margin: 0 2px;">](https://github.com/rhymes-ai/Aria)
- 🎉🎉🎉 2025.1: `ChartMoE` is accepted by **ICLR2025**! 
- 🎉🎉🎉 2025.2: `ChartMoE` is selected as **ICLR2025 Oral(1.8%)**! [<img src="https://img.shields.io/github/stars/IDEA-FinAI/ChartMoE" style="width: auto; height: 20px; vertical-align: top; display: inline; margin: 0 2px;">](https://github.com/IDEA-FinAI/ChartMoE)
<br />
<br />

![Anurag's GitHub stats](https://github-readme-stats.vercel.app/api?username=Coobiw&show_icons=true&theme=rose)
