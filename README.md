### Hi ! Here is Coobiw ğŸ‘‹
<img src="Cyberpunk.jpeg" width = "512" height = "512"  alt="p.s.:Generated by Stable Diffusion" align=center />

<div >
   <em>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp p.s.:Generated by Stable Diffusion</em>
</div>

<br />

#### ğŸ™‹â€â™‚ï¸ About Me:

- ğŸ‘¨â€ğŸ¦° Iâ€™m currently a M.Phil candidate of Peking University. <img src="https://www.pku.edu.cn/pku_logo_red.png" width = "50" height = "40"  align=center />
- â¤ï¸â€ğŸ”¥ Now, I am intersted in Multi-modal Learning especially MLLM.
- ğŸ’¥ In 2023 summer, I take part in [OSPP(Open Source Promotion Plan)](https://summer-ospp.ac.cn/) Summer Camp <img src="https://summer-ospp.ac.cn/vite.svg" width = "60" height = "35"  align=center />, with the honor of contributing for [MMPretrain](https://github.com/open-mmlab/mmpretrain) to build prompt-based classifier. <img src="https://oss.openmmlab.com/www/community/mm.png" width = "60" height = "35"  align=center />
   - Now, the implement of zero-shot CLIP classifier has been merged to the main branch. [PR Link](https://github.com/open-mmlab/mmpretrain/pull/1737)
   - The implement of RAM(Recognize Anything Model) has been merged to the dev branch. Welcome to use the gradio WebUI to test it on MMPretrain! [PR Link](https://github.com/open-mmlab/mmpretrain/pull/1802)
- ğŸ’¥ 2023.10: I implement MiniGPT4Qwen, which is a toy model aligning MiniGPT4 with Qwen-Chat LLM model. I just use 18.8k high quality instruction-tuning data(bi-lingual, selected from minigpt4 and llava). Just fine-tuning the projection layer (3M trainable parameters), this model support Chinese and English! [MiniGPT4Qwen](https://github.com/Coobiw/MiniGPT4Qwen)
<br />
<br />

![Anurag's GitHub stats](https://github-readme-stats.vercel.app/api?username=Coobiw&show_icons=true&theme=rose)
