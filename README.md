### Hi ! Here is Brian Qu *(officially Bowen Qu)*!  ğŸ‘‹

#### ğŸ™‹â€â™‚ï¸ About Me:

- ğŸ‘¨â€ğŸ¦° Iâ€™m currently a Master of Science candidate of Peking University (PKU). <img src="https://www.pku.edu.cn/pku_logo_red.png" width = "50" height = "40"  align=center />
- ğŸ‘¦ Before that, I received the Honours Bachelor, Huazhong University of Science and Technology (HUST).<img src="https://upload.wikimedia.org/wikipedia/zh/thumb/a/ab/Huazhong_University_of_Science_%26_Technology_logo.svg/1920px-Huazhong_University_of_Science_%26_Technology_logo.svg.png" width = "55" height = "40" align=center />
- â¤ï¸â€ğŸ”¥ Now, I am intersted in Multi-modal Learning especially MLLM.

#### ğŸ˜‹ Projects:
- ğŸ’¥ In 2023 summer, I take part in [OSPP(Open Source Promotion Plan)](https://summer-ospp.ac.cn/) Summer Camp <img src="https://summer-ospp.ac.cn/vite.svg" width = "60" height = "35"  align=center />, with the honor of contributing for [MMPretrain](https://github.com/open-mmlab/mmpretrain) to build prompt-based classifier. <img src="https://oss.openmmlab.com/www/community/mm.png" width = "60" height = "35"  align=center />
   - Now, the implement of zero-shot `CLIP` classifier has been merged to the main branch. [![Codebase](https://img.shields.io/badge/PR-Link-pink)](https://github.com/open-mmlab/mmpretrain/pull/1737)
   - The implement of `RAM`(Recognize Anything Model) has been merged to the dev branch. Welcome to use the gradio WebUI to test it on MMPretrain! [![Codebase](https://img.shields.io/badge/PR-Link-pink)](https://github.com/open-mmlab/mmpretrain/pull/1802)
- ğŸ’¥ 2023.11 - 2024.5: `MPP-Qwen-Next` is released! **All training is conducted on 3090/4090 GPUs.** To prevent poverty (24GB of VRAM) from limiting imagination, I implemented an MLLM version based on deepspeed Pipeline Parallel. The Repo supports {video/image/multi-image} {single/multi-turn} conversations. Let's have a try! [![Codebase](https://img.shields.io/badge/Github-Repo-pink)](https://github.com/Coobiw/MPP-LLaVA). <img src="https://github.com/Coobiw/MiniGPT4Qwen/blob/master/assets/MPPQwen/logo.webp" width = "50" height = "50"  align=center /> 
- ğŸ’¥ 2024.9: We release `ChartMoE`, a multimodal large language model with **Mixture-of-Expert connector**, for advanced chart 1)understanding, 2)replot, 3)editing, 4)highlighting and 5)transformation. [![arXiv](https://img.shields.io/badge/ArXiv-Prepint-red)](https://arxiv.org/abs/2409.03277)  [![Project Page](https://img.shields.io/badge/Project-Page-brightgreen)](https://chartmoe.github.io/) [![Codebase](https://img.shields.io/badge/Github-Repo-pink)](https://github.com/IDEA-FinAI/ChartMoE)
- ğŸ’¥ğŸ’¥ğŸ’¥ 2024.10: I am really fortunate to be involved in the development of `Aria`. `Aria` is a **Naive Multimodal MoE** model, with *best-in-class performance across multimodal, language, and coding tasks*! [![Blog](https://img.shields.io/badge/Blog-brightgreen)](https://rhymes.ai/blog) [![Tech Report](https://img.shields.io/badge/Tech-Report-red)](https://arxiv.org/pdf/2410.05993) [![Code](https://img.shields.io/badge/Github-Repo-pink)](https://github.com/rhymes-ai/Aria)
<br />
<br />

![Anurag's GitHub stats](https://github-readme-stats.vercel.app/api?username=Coobiw&show_icons=true&theme=rose)
